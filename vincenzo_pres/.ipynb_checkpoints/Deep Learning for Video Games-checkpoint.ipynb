{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üïπ Deep Learning for Video Games üïπ\n",
    "### ‚≠êÔ∏è Presented by <i>Vincenzo Marcella</i> !!!!\n",
    "\n",
    "* Raytracing (this session) -- denoising\n",
    "* GANs (next session) -- cycle gans, progressive GANs, something something semantic tracing using GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö°Ô∏è Ray tracing\n",
    "\n",
    "Model based off of photons' behavior in _our reality_, which allows for incredibly realistic game graphics\n",
    "\n",
    "**Problem**\n",
    "* Ray tracing is too slow/hard -- _Toy Story_ had 114,240 frames -- worst case scenario could take 4,760 days (on older computers)\n",
    "* Toy Story 4 -- high res -- can take up to 120 hours to render\n",
    "* Hardware has gotten better -- image quality is magnitudes higher; much more complicated environments; many more rays per pixel in order to accommodate\n",
    "* You have to do these insane calculations for every pixel, and also take into account multiple pixels interacting with one another (ex. light reflecting off multiple surfaces)\n",
    "\n",
    "**Solution**\n",
    "* Ray tracing through _denoising_\n",
    "* What we're looking at: MONTE CARLO Rendering\n",
    "\n",
    "### Monte Carlo methods\n",
    "* Monte Carlo integration and path tracing\n",
    "    * Integration\n",
    "        * Used for computing the definite integral using random #s over an interval/range\n",
    "        * In our case, the samples per pixel\n",
    "    * Path tracing\n",
    "        * Create a virtual camera and simulated scene\n",
    "        * Use Monte Carlo Integration over random sample\n",
    "        * Reduce Illuminance to outward light using the bidirectional reflectance distribution function\n",
    "        * Repeat for every point that can be seen by the virtual camera in the simulated scenes\n",
    "        * More samples per point --> better results (i.e. more reduced noise)\n",
    "        * <b>No AI incorporated in this</b> buuuuut\n",
    "        * Path tracing is computationally expensive and it's especially hard when you don't have enough samples per pixel to create realistic images. It's also not temporal, i.e. it doesn't refer to previous scenes to determine what the lighting should be for _this_ scene. We lose 90% of information scene by scene because it's not being reused across time.\n",
    "\n",
    "### Solve the problem with AI!\n",
    "\n",
    "# üß† Recurrent Denoising Autoencoder\n",
    "* Provides:\n",
    "    * Temporal stability\n",
    "    * Real-time ray tracing\n",
    "    * An overall efficient way of producing realistic lighting in 3D environments\n",
    "* Evaluating our model: <b>Convolutional Recurrent Neural Network</b>\n",
    "    * Results need to be temporally stable\n",
    "    * Inputs are very sparse, but recurrent connections gather more information about illumination oevr time\n",
    "    * With our recurrent connections, we can get more valuable info about illumination over time.\n",
    "    * Each step in the encoder and decoder is... combined RNN and CNN layers, if I understand correctly?\n",
    "    * Concatenate upsampled stuff with the data of the same size from a prior \"step\"\n",
    "* Components of model\n",
    "    * ENCODER\n",
    "        * 6 encoding stages\n",
    "        * 6 RCNN blocks (6 stages)\n",
    "        * 3 CNNs per RCNN\n",
    "        * 2x2 max pooling applied at each stage\n",
    "        * Skip connection to corresponding decoder\n",
    "        * 25 total layers (input layer)\n",
    "    * DECODER\n",
    "        * 5 decoding layers\n",
    "        * 2 CNNs per stage\n",
    "        * 2x2 nearest neighbor upsampling applied at every stage\n",
    "        * Concatenates per-pixel feature map from skip connections\n",
    "        * Reconstruct output iamge\n",
    "        * 11 total layers (output layer)\n",
    "* How it works\n",
    "    * ENCODER\n",
    "        * Uses RCNNs to convolute and remember frames from the same sequence to filter noise out\n",
    "        * RCNN block uses one layer to process input feat. from prior layer of encoder and then concatenates the results with features from the previous state\n",
    "        * Skip connections allow OG data to flow from the encoder directly to the decoder\n",
    "    * DECODER\n",
    "        * Upsampling yadda yadda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://research.nvidia.com/sites/default/files/publications/dnn_denoise_author.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
