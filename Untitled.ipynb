{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the TF-IDF matrix for the following documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', 'app', 'belly', 'best', 'came', 'cat', 'chrome', 'climbing', 'eating', 'extension', 'face', 'feedback', 'google', 'impressed', 'incredible', 'key', 'kitten', 'kitty', 'little', 'map', 'merley', 'ninja', 'open', 'photo', 'play', 'promoter', 'restaurant', 'smiley', 'squooshy', 'tab', 'taken', 'translate', 've']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 sentences, and we have 33 vocabulary words. so the dimensions are **8x33**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to decompose the topics into terms of W and H (because ```Tf_IDF = W * H``` -- this is from lin alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics = 2 #let's say we have 2 topics\n",
    "nmf = NMF(n_components=num_of_topics, random_state=1).fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we call it non-negative matrix factorization?\n",
    "\n",
    "* The BoW and TF-IDF matrix elements are all non-negative (zero or positive)\n",
    "* We want to build matrices W and H such that WH returns BoW of TF-IDF matrix\n",
    "* Such that all elements in W and H be non-negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.        , 0.45217213],\n",
       "       [0.55735742, 0.        ],\n",
       "       [0.49414046, 0.        ],\n",
       "       [0.        , 0.74849032],\n",
       "       [0.        , 0.5964714 ],\n",
       "       [0.55735742, 0.        ],\n",
       "       [0.52368298, 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = nmf.fit_transform(tfidf)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 33)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = nmf.components_\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify we can reconstruct TF-IDF matrix from WH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tfidf - np.dot(W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.40824829,\n",
       "          0.        ,  0.        ,  0.        ,  0.40824829,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.40824829,  0.40824829,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.40824829,\n",
       "          0.        ,  0.40824829,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.37700057,  0.19533809,  0.        ,\n",
       "         -0.24333424,  0.        , -0.14642602,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.37700057,  0.        ,  0.        ,  0.        ,\n",
       "          0.37700057, -0.14642602,  0.        , -0.14392194,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.37700057,  0.        ,\n",
       "         -0.14392194,  0.        , -0.14392194],\n",
       "        [-0.10397022,  0.39438703,  0.        ,  0.        ,  0.        ,\n",
       "          0.        , -0.12206152,  0.        ,  0.        , -0.12206152,\n",
       "         -0.10397022, -0.14777505,  0.01304952, -0.14777505,  0.39438703,\n",
       "         -0.12206152,  0.        ,  0.        ,  0.        , -0.14777505,\n",
       "          0.        ,  0.        , -0.10397022,  0.        ,  0.        ,\n",
       "         -0.12206152,  0.        , -0.10397022,  0.        , -0.10397022,\n",
       "          0.        ,  0.39438703,  0.        ],\n",
       "        [ 0.33807186, -0.13101401,  0.        ,  0.        ,  0.        ,\n",
       "          0.        , -0.10821697,  0.        ,  0.        , -0.10821697,\n",
       "          0.33807186, -0.13101401, -0.02040036, -0.13101401, -0.13101401,\n",
       "         -0.10821697,  0.        ,  0.        ,  0.        , -0.13101401,\n",
       "          0.        ,  0.        ,  0.33807186,  0.        ,  0.        ,\n",
       "         -0.10821697,  0.        ,  0.33807186,  0.        ,  0.33807186,\n",
       "          0.        , -0.13101401,  0.        ],\n",
       "        [ 0.        ,  0.        , -0.1392929 ,  0.08292296,  0.        ,\n",
       "         -0.00347388,  0.        , -0.24238216,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        , -0.1392929 ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.1392929 , -0.24238216,  0.        ,  0.2382371 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -0.1392929 ,  0.        ,\n",
       "          0.2382371 ,  0.        ,  0.2382371 ],\n",
       "        [ 0.        ,  0.        , -0.11100241, -0.25213862,  0.        ,\n",
       "          0.1888257 ,  0.        ,  0.41515899,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        , -0.11100241,  0.        ,  0.        ,  0.        ,\n",
       "         -0.11100241,  0.41515899,  0.        , -0.18985097,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -0.11100241,  0.        ,\n",
       "         -0.18985097,  0.        , -0.18985097],\n",
       "        [-0.10397022, -0.14777505,  0.        ,  0.        ,  0.        ,\n",
       "          0.        , -0.12206152,  0.        ,  0.        , -0.12206152,\n",
       "         -0.10397022,  0.39438703,  0.01304952,  0.39438703, -0.14777505,\n",
       "         -0.12206152,  0.        ,  0.        ,  0.        ,  0.39438703,\n",
       "          0.        ,  0.        , -0.10397022,  0.        ,  0.        ,\n",
       "         -0.12206152,  0.        , -0.10397022,  0.        , -0.10397022,\n",
       "          0.        , -0.14777505,  0.        ],\n",
       "        [-0.09768855, -0.13884677,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.36193304,  0.        ,  0.        ,  0.36193304,\n",
       "         -0.09768855, -0.13884677, -0.00852778, -0.13884677, -0.13884677,\n",
       "          0.36193304,  0.        ,  0.        ,  0.        , -0.13884677,\n",
       "          0.        ,  0.        , -0.09768855,  0.        ,  0.        ,\n",
       "          0.36193304,  0.        , -0.09768855,  0.        , -0.09768855,\n",
       "          0.        , -0.13884677,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf - np.dot(W, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the top 10 words from H matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "google feedback map app impressed incredible translate key extension chrome\n",
      "Topic #1:\n",
      "cat best climbing ninja ve photo taken belly merley kitten\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's apply LDA for the same documents we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Modelling with NMF:\n",
      "Topic 0:\n",
      "google feedback map app impressed incredible translate key extension chrome\n",
      "Topic 1:\n",
      "cat best climbing ninja ve photo taken belly merley kitten\n",
      " ---- \n",
      "Topic Modelling with LDA\n",
      "Topic 0:\n",
      "google came kitty restaurant play eating little extension promoter key\n",
      "Topic 1:\n",
      "best cat kitten belly squooshy merley photo ve taken open\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# no_features = 100\n",
    "# NMF is able ot use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 2\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1).fit(tfidf)\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "print('Topic Modelling with NMF:')\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print(' ---- ')\n",
    "print('Topic Modelling with LDA')\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised -- thus, no formal evaluation metrics -- gotta do that ourselves. Yeet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA: Which sentences is what document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04459171 0.95540829]\n",
      " [0.06658954 0.93341046]\n",
      " [0.91159976 0.08840024]\n",
      " [0.95671856 0.04328144]\n",
      " [0.07558235 0.92441765]\n",
      " [0.13248486 0.86751514]\n",
      " [0.1020867  0.8979133 ]\n",
      " [0.08515002 0.91484998]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cherishkim/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "# the dataset to predict on (first 2 samples were also in the training set so one can compare)\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "# Vectorize the training set using the model features as vocabulary\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=2).fit(tf)\n",
    "\n",
    "# transform method returns a matrix with one line per document, columns being topics weight\n",
    "predict = lda.transform(tf)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this work exactly?\n",
    "\n",
    "Not sure. BUT word ORDER is NOT INCORPORATED INTO THIS THING -- so\n",
    "\n",
    "m = 'cat is kitty'\n",
    "n = 'kitty is cat'\n",
    "\n",
    "will have the same shape, because\n",
    "word order is not incorporated in\n",
    "\n",
    "this little is ... cat kitty chrome\n",
    "\n",
    "[0   0         1 .... 1    .... etc ]\n",
    "\n",
    "There are ways to take word order into considerion, but NOT in TF-IDF or word order\n",
    "\n",
    "We can also do some kind of preprocessing to allow for the word order to be taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37686744, 0.62313256]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf_vectorizer.transform(['cat is kitty'])\n",
    "lda.transform(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something about **zero padding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep the word orderings.\n",
    "\n",
    "And we want them to have the same vector length.\n",
    "\n",
    "All models need pre-defined features -- we can first create a dictionary of our vocab, transform it, do zero padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually use keras for deep learning, but here, we're gonna use keras for _text preprocessing_!\n",
    "\n",
    "We are transforming our text into sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.58778666 0.58778666 0.58778666 0.69314718 0.84729786\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.58778666 0.58778666 0.58778666 1.17360019 0.\n",
      "  1.09861229 0.         0.         0.        ]\n",
      " [0.         0.58778666 0.58778666 0.58778666 0.         0.\n",
      "  0.         1.09861229 1.09861229 1.09861229]\n",
      " [0.         0.58778666 0.58778666 0.58778666 0.69314718 0.84729786\n",
      "  0.         0.         0.         0.        ]]\n",
      "[[1, 2, 3, 5, 4], [1, 4, 2, 3, 6, 4], [7, 1, 2, 3, 8, 9], [2, 1, 3, 5, 4]]\n",
      "Word Index (This is a DICTIONARY)\n",
      "{'this': 1, 'is': 2, 'the': 3, 'document': 4, 'first': 5, 'second': 6, 'and': 7, 'third': 8, 'one': 9}\n",
      "Word Counts:\n",
      "OrderedDict([('this', 4), ('is', 4), ('the', 4), ('first', 2), ('document', 4), ('second', 1), ('and', 1), ('third', 1), ('one', 1)])\n",
      "Document Count:\n",
      "4\n",
      "Words in Doc:\n",
      "defaultdict(<class 'int'>, {'the': 4, 'this': 4, 'is': 4, 'first': 2, 'document': 3, 'second': 1, 'one': 1, 'third': 1, 'and': 1})\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "documents = ['This is the first document.',\n",
    "            'This document is the second document.',\n",
    "            'And this is the third one.',\n",
    "            'Is this the first document?']\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(documents)\n",
    "mat_texts = tok.texts_to_matrix(documents, mode='tfidf')\n",
    "print(mat_texts)\n",
    "X = tok.texts_to_sequences(documents)\n",
    "print(X)\n",
    "print('Word Index (This is a DICTIONARY)')\n",
    "print(tok.word_index)\n",
    "print('Word Counts:')\n",
    "print(tok.word_counts)\n",
    "print('Document Count:')\n",
    "print(tok.document_count)\n",
    "print('Words in Doc:')\n",
    "print(tok.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
